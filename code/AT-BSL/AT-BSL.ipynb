{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0bf82-43f7-4945-82ab-ac4715cc8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "\n",
    "class Cifar10LT(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = np.transpose(np.reshape(img, (3, 32, 32)), (1, 2, 0))  # Convert to C,H,W format\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Set save directory\n",
    "#save_dir = '/home/u2023170724/jupyterlab/ADAT1/Icifar10/cifar10lt_10'\n",
    "save_dir = '../../datasets/cifar10lt_10'\n",
    "\n",
    "# Load training and test sets\n",
    "train_images = np.load(os.path.join(save_dir, 'train_images.npy'))\n",
    "train_labels = np.load(os.path.join(save_dir, 'train_labels.npy'))\n",
    "test_images = np.load(os.path.join(save_dir, 'test_images.npy'))\n",
    "test_labels = np.load(os.path.join(save_dir, 'test_labels.npy'))\n",
    "\n",
    "# Data preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  \n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.RandomCrop(224, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  \n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Cifar10LT(images=train_images, labels=train_labels, transform=train_transform)\n",
    "test_dataset = Cifar10LT(images=test_images, labels=test_labels, transform=test_transform)\n",
    "\n",
    "# Create DataLoader\n",
    "#train_loader = DataLoader(train_dataset, batch_size=128, num_workers=3, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=128, num_workers=3, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, num_workers=0, shuffle=False)\n",
    "dataloaders = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "unique_train_labels, train_counts = np.unique(train_dataset.labels, return_counts=True)\n",
    "print(\"Number of samples per class in training set:\", dict(zip(unique_train_labels, train_counts)))\n",
    "\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "unique_test_labels, test_counts = np.unique(test_dataset.labels, return_counts=True)\n",
    "print(\"Number of samples per class in test set:\", dict(zip(unique_test_labels, test_counts)))\n",
    "\n",
    "# Example: Iterate through training data loader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Shape of batch images in training set:\", images.shape)\n",
    "    print(\"Shape of batch labels in training set:\", labels.shape)\n",
    "    break  # Only show the first batch\n",
    "\n",
    "# Example: Iterate through test data loader\n",
    "for images, labels in test_loader:\n",
    "    print(\"Shape of batch images in test set:\", images.shape)\n",
    "    print(\"Shape of batch labels in test set:\", labels.shape)\n",
    "    break  # Only show the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680f043-c5b0-4a1c-a425-56af6265114a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(weights=None)\n",
    "#model = resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee200fc-d74f-4dbe-8b94-21770ecdd2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = model.fc.in_features  # Get the number of input features\n",
    "model.fc = nn.Linear(num_features, 10)  # Change the output features to 10 (for CIFAR-10 classes)\n",
    "model.to(device)  # Move the model to the specified device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d202c43-139c-46c9-ac64-97990a023486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SGD optimizer with learning rate 0.1, momentum 0.9 and weight decay 5e-4\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Setup learning rate scheduler that reduces the learning rate at epochs 75 and 90\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 90], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217c032-0c1f-4577-8a4c-bdba889be354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def BSL(labels, logits, sample_per_class):\n",
    "    spc = torch.tensor(sample_per_class).type_as(logits)  # Convert sample count to same data type as logits\n",
    "    spc = spc.unsqueeze(0).expand(logits.shape[0], -1)    # Expand to the same dimension as logits\n",
    "    logits = logits + spc.log()                           # Add the log of sample counts to the logits\n",
    "    loss = F.cross_entropy(input=logits, target=labels)   # Calculate weighted cross entropy loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def pgd_loss(model,\n",
    "                x_natural,\n",
    "                y,\n",
    "                samples_per_cls,\n",
    "                optimizer,\n",
    "                step_size=0.003,\n",
    "                epsilon=0.031,\n",
    "                perturb_steps=10,\n",
    "                distance='l_inf'):\n",
    "    model.eval()\n",
    "    # Generate adversarial example\n",
    "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
    "    if distance == 'l_inf':\n",
    "        for _ in range(perturb_steps):\n",
    "            x_adv.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                loss_ce = F.cross_entropy(model(x_adv), y)\n",
    "            grad = torch.autograd.grad(loss_ce, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "            x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
    "    # Zero gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = BSL(y, model(x_adv), samples_per_cls)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846d271-3057-45d5-9996-d90753171893",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.003\n",
    "epsilon = 0.031\n",
    "perturb_steps = 10\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_stats(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Calculate the number of samples per class and the total number of classes for the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: PyTorch Dataset instance (must have a 'targets' attribute or similar)\n",
    "\n",
    "    Returns:\n",
    "    - samples_per_cls: Number of samples per class\n",
    "    - no_of_classes: Total number of classes\n",
    "    \"\"\"\n",
    "    # Check if dataset has a 'targets' attribute\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = dataset.targets.cpu().numpy() if isinstance(dataset.targets, torch.Tensor) else np.array(dataset.targets)\n",
    "    else:\n",
    "        # Manually extract targets if the dataset does not have a 'targets' attribute\n",
    "        targets = np.array([target for _, target in dataset])\n",
    "\n",
    "    no_of_classes = len(np.unique(targets))\n",
    "    samples_per_cls = np.array([np.sum(targets == i) for i in range(no_of_classes)])\n",
    "    \n",
    "    return samples_per_cls, no_of_classes\n",
    "\n",
    "# Calculate samples_per_cls and no_of_classes for train_dataset\n",
    "samples_per_cls, no_of_classes = calculate_class_stats(train_dataset)\n",
    "print(f'Samples per class: {samples_per_cls}')\n",
    "print(f'Number of classes: {no_of_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116a2e4-2b29-4a88-b8b0-631a4c03b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchattacks import FGSM, PGD, CW, DeepFool, AutoAttack\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume test_dataset and model are already defined\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10  # Assume there are 10 classes\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def evaluate_model_with_attack_by_class(model, dataloader, attack, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on each class using adversarial attacks, calculating accuracy only.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be evaluated\n",
    "    - dataloader: Data loader providing test data\n",
    "    - attack: Adversarial attack object\n",
    "    - num_classes: Number of classes in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Add a progress bar around the dataloader\n",
    "    for images, labels in tqdm(dataloader, desc=f'Evaluating {attack}'):\n",
    "        labels = labels.cuda()\n",
    "        images = attack(images, labels).cpu()\n",
    "        outputs = model(images.cuda())\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Record results by class\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = {}\n",
    "    for class_idx in range(num_classes):\n",
    "        if class_total[class_idx] > 0:\n",
    "            class_accuracies[class_idx] = class_correct[class_idx] / class_total[class_idx]\n",
    "        else:\n",
    "            class_accuracies[class_idx] = None\n",
    "\n",
    "    # Calculate overall average accuracy\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Calculate class-balanced accuracy\n",
    "    balanced_accuracy = np.mean([acc for acc in class_accuracies.values() if acc is not None])\n",
    "\n",
    "    return class_accuracies, avg_accuracy, balanced_accuracy\n",
    "\n",
    "\n",
    "def save_result_to_csv(attack_name, class_accuracies, avg_accuracy, balanced_accuracy, filename='attack_results.csv'):\n",
    "    \"\"\"\n",
    "    Save attack results by class and overall average results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - attack_name: Name of the attack\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    - filename: Name of the output file\n",
    "    \"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write accuracy for each class\n",
    "        for class_idx, accuracy in class_accuracies.items():\n",
    "            writer.writerow([f'{attack_name} - Class {class_idx}', accuracy])\n",
    "        \n",
    "        # Write overall average accuracy and class-balanced accuracy\n",
    "        writer.writerow([f'{attack_name} - Avg Accuracy', avg_accuracy, ''])\n",
    "        writer.writerow([f'{attack_name} - Balanced Accuracy', balanced_accuracy, ''])\n",
    "        print(f'{attack_name} - Avg Accuracy: {avg_accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f} saved to {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66357656-b78e-4349-b27f-83324a671cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, scheduler, num_epochs=100):\n",
    "    train_loss_history = []\n",
    "\n",
    "    save_dir = os.getcwd()\n",
    "    param_dir = os.path.join(save_dir, \"parm\")\n",
    "    os.makedirs(param_dir, exist_ok=True)  # Create directory for saving parameters\n",
    "\n",
    "    # Initialize training loss CSV file\n",
    "    csv_path = os.path.join(save_dir, \"train_loss.csv\")\n",
    "    with open(csv_path, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Epoch', 'Train Loss'])\n",
    "\n",
    "    # Initialize attack results CSV file\n",
    "    attack_results_path = os.path.join(save_dir, \"attack_results.csv\")\n",
    "    with open(attack_results_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Attack', 'Class Index', 'Accuracy'])  # Header row with Epoch column\n",
    "\n",
    "    tz = pytz.timezone('Asia/Shanghai')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(dataloaders['train'], desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets = targets.squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = pgd_loss(model, inputs, targets, samples_per_cls, optimizer, step_size, epsilon, perturb_steps)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders['train'].dataset)\n",
    "        print('Train Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print('Time taken: {:.0f}m {:.0f}s'.format(epoch_time // 60, epoch_time % 60))\n",
    "\n",
    "        current_time_beijing = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print('Current time (Beijing):', current_time_beijing)\n",
    "\n",
    "        train_loss_history.append(epoch_loss)\n",
    "\n",
    "        with open(csv_path, \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([epoch + 1, epoch_loss])\n",
    "        \n",
    "        # Save parameters for current epoch\n",
    "        param_file_path = os.path.join(param_dir, f\"model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), param_file_path)\n",
    "\n",
    "        # Call evaluation function and save results\n",
    "        if (epoch + 1) % 10 == 0:  # Check if current epoch is multiple of 10\n",
    "            evaluate_and_save_results(model, test_loader, attack_results_path, epoch + 1)  # Pass current epoch\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save final model parameters\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"model_final.pt\"))\n",
    "\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_history, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_save_results(model, test_loader, results_path, epoch):\n",
    "    # Define attacks to evaluate\n",
    "    attacks = [\n",
    "        ('nature', FGSM(model, eps=0)),\n",
    "        ('PGD-20', PGD(model, eps=8/255, alpha=1/255, steps=20)),\n",
    "    ]\n",
    "\n",
    "    for attack_name, attack in attacks:\n",
    "        print(f'Evaluating with {attack_name}...')\n",
    "        class_accuracies, avg_accuracy, balanced_accuracy = evaluate_model_with_attack_by_class(model, test_loader, attack, NUM_CLASSES)\n",
    "\n",
    "        # Save attack results\n",
    "        with open(results_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for class_idx, accuracy in class_accuracies.items():\n",
    "                writer.writerow([epoch, attack_name, class_idx, accuracy])  # Write current epoch, attack name, class index and accuracy\n",
    "            \n",
    "            # Write overall average and balanced accuracy\n",
    "            writer.writerow([epoch, attack_name, 'Avg Accuracy', avg_accuracy])\n",
    "            writer.writerow([epoch, attack_name, 'Balanced Accuracy', balanced_accuracy])\n",
    "            print(f'{attack_name} - Avg Accuracy: {avg_accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f} saved to {results_path}')\n",
    "\n",
    "# Other related functions remain unchanged...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87b6f3-eeb2-4cdd-a64a-ee541962480b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Start the model train processing\n",
    "train_model(model, dataloaders, optimizer, scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3dedbc-9893-4bc3-b6c1-0f628abd2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets = targets.squeeze()\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            total_samples += targets.size(0)\n",
    "            total_correct += (predicted == targets).sum().item()  # Count correct predictions\n",
    "            # Store true labels and prediction scores for ROC AUC calculation\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            softmax_probs = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "            y_scores.extend(softmax_probs.cpu().numpy())\n",
    "        \n",
    "    # Calculate metrics\n",
    "    accuracy = total_correct / total_samples\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "    # Calculate ROC AUC for multi-class classification\n",
    "    roc_auc = roc_auc_score(y_true, y_scores, multi_class='ovo', average='macro')\n",
    "    \n",
    "    return accuracy, roc_auc\n",
    "\n",
    "# Evaluate on training data\n",
    "accuracy, roc_auc = evaluate_model(model, train_loader)\n",
    "print(f\"Train Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Train ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy, roc_auc = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d588c-30b4-44ea-b62f-4ca9f361b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "from torchattacks import PGD, FGSM\n",
    "\n",
    "# Set batch size for testing\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448fcc95-0400-4926-9b01-c8c273c33d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the saved weights file\n",
    "weights_path = './model_final.pt'\n",
    "# Load the state dictionary from the file\n",
    "state_dict = torch.load(weights_path)\n",
    "# Update the model's parameters with the loaded state dictionary\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f60a8-d32b-4415-a268-93bab59d3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchattacks import FGSM, PGD, CW, DeepFool, AutoAttack\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming test_dataset and model are already defined\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10  # Assuming 10 classes\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def evaluate_model_with_attack_by_class(model, dataloader, attack, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on each class using the given adversarial attack, calculating accuracy only.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to evaluate\n",
    "    - dataloader: Data loader providing test data\n",
    "    - attack: Adversarial attack object\n",
    "    - num_classes: Number of classes in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Add a progress bar around the dataloader\n",
    "    for images, labels in tqdm(dataloader, desc=f'Evaluating {attack}'):\n",
    "        labels = labels.cuda()\n",
    "        images = attack(images, labels).cpu()\n",
    "        outputs = model(images.cuda())\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Record results by class\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = {}\n",
    "    for class_idx in range(num_classes):\n",
    "        if class_total[class_idx] > 0:\n",
    "            class_accuracies[class_idx] = class_correct[class_idx] / class_total[class_idx]\n",
    "        else:\n",
    "            class_accuracies[class_idx] = None\n",
    "\n",
    "    # Calculate overall average accuracy\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Calculate class-balanced accuracy\n",
    "    balanced_accuracy = np.mean([acc for acc in class_accuracies.values() if acc is not None])\n",
    "\n",
    "    return class_accuracies, avg_accuracy, balanced_accuracy\n",
    "\n",
    "\n",
    "def save_result_to_csv(attack_name, class_accuracies, avg_accuracy, balanced_accuracy, filename='attack_results.csv'):\n",
    "    \"\"\"\n",
    "    Save attack results by class and overall average to CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - attack_name: Name of the attack\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    - filename: Name of file to save\n",
    "    \"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write accuracy for each class\n",
    "        for class_idx, accuracy in class_accuracies.items():\n",
    "            writer.writerow([f'{attack_name} - Class {class_idx}', accuracy])\n",
    "        \n",
    "        # Write overall average accuracy and class-balanced accuracy\n",
    "        writer.writerow([f'{attack_name} - Avg Accuracy', avg_accuracy, ''])\n",
    "        writer.writerow([f'{attack_name} - Balanced Accuracy', balanced_accuracy, ''])\n",
    "        print(f'{attack_name} - Avg Accuracy: {avg_accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f} saved to {filename}')\n",
    "\n",
    "\n",
    "# Initialize CSV file with header\n",
    "with open('attack_results_final.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Attack', 'Accuracy'])\n",
    "\n",
    "# Define attacks to evaluate\n",
    "attacks = [\n",
    "    ('nature', FGSM(model, eps=0)),\n",
    "    ('FGSM', FGSM(model, eps=8/255)),\n",
    "    ('PGD-20', PGD(model, eps=8/255, alpha=1/255, steps=20)),\n",
    "    ('PGD-100', PGD(model, eps=8/255, alpha=1/255, steps=100)),\n",
    "    ('CW', CW(model, c=10, kappa=0, steps=100, lr=0.01)),\n",
    "    ('DeepFool', DeepFool(model)),\n",
    "    ('AutoAttack', AutoAttack(model, norm='Linf', eps=8/255, version='standard', n_classes=NUM_CLASSES, seed=None, verbose=False))\n",
    "]\n",
    "\n",
    "# Evaluate each attack and save results in real-time\n",
    "for attack_name, attack in attacks:\n",
    "    print(f'Evaluating with {attack_name}...')\n",
    "    class_accuracies, avg_accuracy, balanced_accuracy = evaluate_model_with_attack_by_class(model, test_loader, attack, NUM_CLASSES)\n",
    "    save_result_to_csv(attack_name, class_accuracies, avg_accuracy, balanced_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe9a36-b1a3-4179-9e29-ca7814bb0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchattacks import FGSM, PGD, CW, DeepFool, AutoAttack\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume test_dataset and model are already defined\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10  # Assume there are 10 classes\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def evaluate_model_with_attack_by_class(model, dataloader, attack, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on each class using adversarial attacks, calculating accuracy only.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be evaluated\n",
    "    - dataloader: Data loader providing test data\n",
    "    - attack: Adversarial attack object\n",
    "    - num_classes: Number of classes in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Add a progress bar around the dataloader\n",
    "    for images, labels in tqdm(dataloader, desc=f'Evaluating {attack}'):\n",
    "        labels = labels.cuda()\n",
    "        images = attack(images, labels).cpu()\n",
    "        outputs = model(images.cuda())\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Record results by class\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = {}\n",
    "    for class_idx in range(num_classes):\n",
    "        if class_total[class_idx] > 0:\n",
    "            class_accuracies[class_idx] = class_correct[class_idx] / class_total[class_idx]\n",
    "        else:\n",
    "            class_accuracies[class_idx] = None\n",
    "\n",
    "    # Calculate overall average accuracy\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Calculate class-balanced accuracy\n",
    "    balanced_accuracy = np.mean([acc for acc in class_accuracies.values() if acc is not None])\n",
    "\n",
    "    return class_accuracies, avg_accuracy, balanced_accuracy\n",
    "\n",
    "\n",
    "def save_result_to_csv(attack_name, class_accuracies, avg_accuracy, balanced_accuracy, filename='attack_results.csv'):\n",
    "    \"\"\"\n",
    "    Save attack results by class and overall average results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - attack_name: Name of the attack\n",
    "    - class_accuracies: Accuracy for each class\n",
    "    - avg_accuracy: Overall average accuracy\n",
    "    - balanced_accuracy: Class-balanced accuracy\n",
    "    - filename: Name of the output file\n",
    "    \"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write accuracy for each class\n",
    "        for class_idx, accuracy in class_accuracies.items():\n",
    "            writer.writerow([f'{attack_name} - Class {class_idx}', accuracy])\n",
    "        \n",
    "        # Write overall average accuracy and class-balanced accuracy\n",
    "        writer.writerow([f'{attack_name} - Avg Accuracy', avg_accuracy, ''])\n",
    "        writer.writerow([f'{attack_name} - Balanced Accuracy', balanced_accuracy, ''])\n",
    "        print(f'{attack_name} - Avg Accuracy: {avg_accuracy:.4f}, Balanced Accuracy: {balanced_accuracy:.4f} saved to {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b59e11-f89b-4248-aecb-2fae3ae8bf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfvsn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
